{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAoPIlUCrcxw"
      },
      "source": [
        "# Assignment 3a: Basic Gradio RAG Frontend\n",
        "## Day 6 Session 2 - Building Simple RAG Applications\n",
        "\n",
        "In this assignment, you'll build a simple Gradio frontend for your RAG system with just the essential features:\n",
        "- Button to initialize the vector database\n",
        "- Search query input and button\n",
        "- Display of AI responses\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Create basic Gradio interfaces\n",
        "- Connect RAG backend to frontend\n",
        "- Handle user interactions and database initialization\n",
        "- Build functional AI-powered web applications\n",
        "\n",
        "**Prerequisites:**\n",
        "- Completed Assignment 1 (Vector Database Basics)\n",
        "- Completed Assignment 2 (Advanced RAG)\n",
        "- Understanding of LlamaIndex fundamentals\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9X_9cCorcx0"
      },
      "source": [
        "## ðŸ“š Part 1: Setup and Imports\n",
        "\n",
        "Import all necessary libraries for building your Gradio RAG application.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LB 20251208\n",
        "!pwd\n",
        "!git clone https://github.com/lakshmananb/ai-accelerator-C3/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kROLHSL4GDp",
        "outputId": "06e544d7-c0a8-4689-cec0-8ef4ba1e5bf6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'ai-accelerator-C3'...\n",
            "remote: Enumerating objects: 161, done.\u001b[K\n",
            "remote: Counting objects: 100% (161/161), done.\u001b[K\n",
            "remote: Compressing objects: 100% (143/143), done.\u001b[K\n",
            "remote: Total 161 (delta 34), reused 73 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (161/161), 26.08 MiB | 30.76 MiB/s, done.\n",
            "Resolving deltas: 100% (34/34), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "!ls /content/ai-accelerator-C3/Day_7/session_2/\n",
        "!pip install -r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "s3SIOF1f5E7K",
        "outputId": "ef942e39-81b1-41c5-af4b-f3f6bca938ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ai-accelerator-C3  sample_data\n",
            "assignments  llamaindex_rag  prompt_samples    youtube_video\n",
            "data\t     papers\t     requirements.txt\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from -r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 1)) (4.13.5)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from -r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 2)) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from -r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 3)) (2.187.0)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.12/dist-packages (from -r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 4)) (2.43.0)\n",
            "Requirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.12/dist-packages (from -r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 5)) (0.2.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (from -r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (5.50.0)\n",
            "Requirement already satisfied: gradio_client in /usr/local/lib/python3.12/dist-packages (from -r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 7)) (1.14.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from -r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 8)) (0.36.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.12/dist-packages (from -r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 9)) (6.17.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.12/dist-packages (from -r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 10)) (7.34.0)\n",
            "Collecting lancedb (from -r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 11))\n",
            "  Downloading lancedb-0.25.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting llama-index (from -r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_index-0.14.10-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting llama-index-vector-stores-lancedb (from -r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 13))\n",
            "  Downloading llama_index_vector_stores_lancedb-0.4.2-py3-none-any.whl.metadata (460 bytes)\n",
            "Collecting llama-index-embeddings-huggingface (from -r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 14))\n",
            "  Downloading llama_index_embeddings_huggingface-0.6.1-py3-none-any.whl.metadata (458 bytes)\n",
            "Collecting llama-index-llms-huggingface-api (from -r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 15))\n",
            "  Downloading llama_index_llms_huggingface_api-0.6.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting llama-index-embeddings-openai (from -r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 16))\n",
            "  Downloading llama_index_embeddings_openai-0.5.1-py3-none-any.whl.metadata (400 bytes)\n",
            "Collecting llama-index-llms-openrouter (from -r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 17))\n",
            "  Downloading llama_index_llms_openrouter-0.4.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from -r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 18)) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 19)) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 20)) (2.2.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (from -r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 21)) (2.9.0)\n",
            "Collecting openai-whisper (from -r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 22))\n",
            "  Downloading openai_whisper-20250625.tar.gz (803 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from -r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 23)) (2.12.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (from -r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 24)) (5.1.2)\n",
            "Collecting yt-dlp (from -r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 25))\n",
            "  Downloading yt_dlp-2025.12.8-py3-none-any.whl.metadata (180 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (from -r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 26)) (3.8.11)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 1)) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 2)) (1.72.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 2)) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 2)) (1.26.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 2)) (2.32.4)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 3)) (0.31.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 3)) (4.2.0)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 4)) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 4)) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 4)) (4.9.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (4.12.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (0.118.3)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (1.0.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (3.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (25.0)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (11.3.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (0.14.8)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (0.20.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio_client->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 7)) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio_client->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 7)) (15.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 8)) (3.20.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 8)) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 8)) (1.2.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 9)) (1.8.15)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 9)) (7.4.9)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 9)) (0.2.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 9)) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 9)) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from ipykernel->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 9)) (26.2.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 9)) (6.5.1)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 9)) (5.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 10)) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 10))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 10)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 10)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 10)) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 10)) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 10)) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 10)) (4.9.0)\n",
            "Requirement already satisfied: deprecation in /usr/local/lib/python3.12/dist-packages (from lancedb->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 11)) (2.1.0)\n",
            "Requirement already satisfied: pyarrow>=16 in /usr/local/lib/python3.12/dist-packages (from lancedb->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 11)) (18.1.0)\n",
            "Collecting lance-namespace>=0.0.16 (from lancedb->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 11))\n",
            "  Downloading lance_namespace-0.2.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting llama-index-cli<0.6,>=0.5.0 (from llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_index_cli-0.5.3-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting llama-index-core<0.15.0,>=0.14.10 (from llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_index_core-0.14.10-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.9.4-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-index-llms-openai<0.7,>=0.6.0 (from llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_index_llms_openai-0.6.10-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting llama-index-readers-file<0.6,>=0.5.0 (from llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_index_readers_file-0.5.5-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_index_readers_llama_parse-0.5.1-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting pylance (from llama-index-vector-stores-lancedb->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 13))\n",
            "  Downloading pylance-0.39.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting tantivy (from llama-index-vector-stores-lancedb->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 13))\n",
            "  Downloading tantivy-0.25.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (336 bytes)\n",
            "Collecting llama-index-llms-openai-like<0.6,>=0.5.0 (from llama-index-llms-openrouter->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 17))\n",
            "  Downloading llama_index_llms_openai_like-0.5.3-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 18)) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 18)) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 18)) (2025.11.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 20)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 20)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 20)) (2025.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 21)) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 21)) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 21)) (1.3.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 22)) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 22)) (0.60.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 22)) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 22)) (2.9.0+cu126)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 22)) (3.5.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 23)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 23)) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 23)) (0.4.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 24)) (4.57.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 24)) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 24)) (1.16.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 26)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 26)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 26)) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 26)) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 26)) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 26)) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 26)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 26)) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 26)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 26)) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 26)) (0.20.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (3.11)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 3)) (3.2.5)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (0.16.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 14)) (3.13.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 10)) (0.8.5)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 9)) (0.4)\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 9)) (5.9.1)\n",
            "Collecting lance-namespace-urllib3-client (from lance-namespace>=0.0.16->lancedb->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 11))\n",
            "  Downloading lance_namespace_urllib3_client-0.2.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12)) (0.21.0)\n",
            "Collecting banks<3,>=2.2.0 (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading banks-2.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dataclasses-json (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading deprecated-1.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2,>=1.2.0 (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting llama-index-workflows!=2.9.0,<3,>=2 (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_index_workflows-2.11.5-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12)) (3.6)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12)) (4.5.1)\n",
            "Collecting setuptools>=18.5 (from ipython->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 10))\n",
            "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.10->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12)) (2.0.44)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12)) (9.1.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12)) (2.0.1)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting llama-cloud==0.1.35 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud-0.1.35-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting wrapt (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12)) (0.7.1)\n",
            "Collecting pypdf<7,>=6.1.3 (from llama-index-readers-file<0.6,>=0.5.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading pypdf-6.4.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.6,>=0.5.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.88-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 10)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 10)) (0.2.14)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 4)) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 20)) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 2)) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 2)) (2.5.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 26)) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 26)) (0.1.5)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 22)) (1.14.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 22)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 22)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 22)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 22)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 22)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 22)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 22)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 22)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 22)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 22)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 22)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 22)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 22)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 22)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 22)) (1.11.1.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 24)) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 24)) (0.7.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 26)) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 26)) (7.5.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 22)) (0.43.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 24)) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 14)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 14)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 14)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 14)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 14)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 14)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 14)) (1.22.0)\n",
            "Collecting griffe (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.10->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading griffe-1.15.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting llama-index-instrumentation>=0.1.0 (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15.0,>=0.14.10->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_index_instrumentation-0.4.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting llama-cloud-services>=0.6.88 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.88-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (4.0.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.10->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12)) (3.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 22)) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.15.0,>=0.14.10->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.15.0,>=0.14.10->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "INFO: pip is looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.87-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.87 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.87-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.86-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.86 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.86-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.85-py3-none-any.whl.metadata (6.6 kB)\n",
            "INFO: pip is still looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-cloud-services>=0.6.85 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.85-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.84-py3-none-any.whl.metadata (6.6 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting llama-cloud-services>=0.6.84 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.84-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.83-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.82 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.83-py3-none-any.whl.metadata (3.3 kB)\n",
            "  Downloading llama_cloud_services-0.6.82-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.82-py3-none-any.whl.metadata (6.6 kB)\n",
            "  Downloading llama_parse-0.6.81-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.81 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.81-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.80-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.80 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.80-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.79-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.79 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.79-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.78-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.78 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.78-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.77-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.77 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.77-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.76-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.76 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.76-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.75-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.75 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.75-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.74-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.74 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.74-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.73-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.73 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.73-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.72-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.72 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.72-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.71-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.71 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.71-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.70-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.70 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.70-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.69-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.69 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.69-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.68-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.68 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.68-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.67-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.67 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.67-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.66-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.66 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.66-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.65-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.64 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.65-py3-none-any.whl.metadata (3.3 kB)\n",
            "  Downloading llama_cloud_services-0.6.64-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.64-py3-none-any.whl.metadata (6.6 kB)\n",
            "  Downloading llama_parse-0.6.63-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.63 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.63-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.62-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.62 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.62-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.60-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.60 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.60-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.59-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.59 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.59-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.58-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.58 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.58-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.57-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.56 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.57-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading llama_cloud_services-0.6.56-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.56-py3-none-any.whl.metadata (6.6 kB)\n",
            "  Downloading llama_parse-0.6.55-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.55 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.55-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.54-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.54 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.54-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv<2,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-cloud-services>=0.6.54->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12)) (1.2.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 6)) (0.1.2)\n",
            "Collecting colorama>=0.4 (from griffe->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.10->llama-index->-r /content/ai-accelerator-C3/Day_7/session_2/requirements.txt (line 12))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Downloading lancedb-0.25.3-cp39-abi3-manylinux_2_28_x86_64.whl (39.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.2/39.2 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index-0.14.10-py3-none-any.whl (7.5 kB)\n",
            "Downloading llama_index_vector_stores_lancedb-0.4.2-py3-none-any.whl (7.9 kB)\n",
            "Downloading llama_index_embeddings_huggingface-0.6.1-py3-none-any.whl (8.9 kB)\n",
            "Downloading llama_index_llms_huggingface_api-0.6.1-py3-none-any.whl (7.5 kB)\n",
            "Downloading llama_index_embeddings_openai-0.5.1-py3-none-any.whl (7.0 kB)\n",
            "Downloading llama_index_llms_openrouter-0.4.2-py3-none-any.whl (4.5 kB)\n",
            "Downloading yt_dlp-2025.12.8-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lance_namespace-0.2.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading llama_index_cli-0.5.3-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_core-0.14.10-py3-none-any.whl (11.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_indices_managed_llama_cloud-0.9.4-py3-none-any.whl (17 kB)\n",
            "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading llama_cloud-0.1.35-py3-none-any.whl (303 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.6.10-py3-none-any.whl (26 kB)\n",
            "Downloading llama_index_llms_openai_like-0.5.3-py3-none-any.whl (4.7 kB)\n",
            "Downloading llama_index_readers_file-0.5.5-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_readers_llama_parse-0.5.1-py3-none-any.whl (3.2 kB)\n",
            "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "Downloading pylance-0.39.0-cp39-abi3-manylinux_2_28_x86_64.whl (48.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.6/48.6 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tantivy-0.25.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading banks-2.2.0-py3-none-any.whl (29 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_index_workflows-2.11.5-py3-none-any.whl (91 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.6.54-py3-none-any.whl (4.9 kB)\n",
            "Downloading llama_cloud_services-0.6.54-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.4.1-py3-none-any.whl (328 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m328.3/328.3 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (88 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading lance_namespace_urllib3_client-0.2.1-py3-none-any.whl (228 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m228.5/228.5 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_instrumentation-0.4.2-py3-none-any.whl (15 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading griffe-1.15.0-py3-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=b28183a3f85b4d31b35bbf03045c253b89711ec8f9c62581ad5c507d98529ff7\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/d2/20/09ec9bef734d126cba375b15898010b6cc28578d8afdde5869\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: striprtf, filetype, dirtyjson, yt-dlp, wrapt, tantivy, setuptools, pypdf, mypy-extensions, marshmallow, jedi, colorama, typing-inspect, griffe, deprecated, llama-index-instrumentation, llama-cloud, lance-namespace-urllib3-client, dataclasses-json, banks, openai-whisper, llama-index-workflows, lance-namespace, pylance, llama-index-core, lancedb, llama-index-vector-stores-lancedb, llama-index-readers-file, llama-index-llms-openai, llama-index-llms-huggingface-api, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-embeddings-huggingface, llama-cloud-services, llama-parse, llama-index-llms-openai-like, llama-index-cli, llama-index-readers-llama-parse, llama-index-llms-openrouter, llama-index\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 2.0.1\n",
            "    Uninstalling wrapt-2.0.1:\n",
            "      Successfully uninstalled wrapt-2.0.1\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "Successfully installed banks-2.2.0 colorama-0.4.6 dataclasses-json-0.6.7 deprecated-1.2.18 dirtyjson-1.0.8 filetype-1.2.0 griffe-1.15.0 jedi-0.19.2 lance-namespace-0.2.1 lance-namespace-urllib3-client-0.2.1 lancedb-0.25.3 llama-cloud-0.1.35 llama-cloud-services-0.6.54 llama-index-0.14.10 llama-index-cli-0.5.3 llama-index-core-0.14.10 llama-index-embeddings-huggingface-0.6.1 llama-index-embeddings-openai-0.5.1 llama-index-indices-managed-llama-cloud-0.9.4 llama-index-instrumentation-0.4.2 llama-index-llms-huggingface-api-0.6.1 llama-index-llms-openai-0.6.10 llama-index-llms-openai-like-0.5.3 llama-index-llms-openrouter-0.4.2 llama-index-readers-file-0.5.5 llama-index-readers-llama-parse-0.5.1 llama-index-vector-stores-lancedb-0.4.2 llama-index-workflows-2.11.5 llama-parse-0.6.54 marshmallow-3.26.1 mypy-extensions-1.1.0 openai-whisper-20250625 pylance-0.39.0 pypdf-6.4.1 setuptools-80.9.0 striprtf-0.0.26 tantivy-0.25.1 typing-inspect-0.9.0 wrapt-1.17.3 yt-dlp-2025.12.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack"
                ]
              },
              "id": "fbe1dd926ef845e892d31215a9b783ef"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LB 20251208:  Below line added for Openrouter API Key configuration (OutSkill provided one fails at Simiarity filtering stage)\n",
        "# LB 20251208:  API key fixed by generating new personal key\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"OPENROUTER_API_KEY\"] = getpass(\"\") #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 (Outskill) / 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 (Personal)\n",
        "print(\"âœ“ OpenrRouter key set successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFB_HaZ6IX_z",
        "outputId": "2f64f9ec-d6f5-409c-9176-5e770360e3fe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "âœ“ OpenrRouter key set successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VA1t9bFarcx2",
        "outputId": "5c0e6bbe-1813-45dd-ad77-bb02c43629ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import gradio as gr\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# LlamaIndex components\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, Settings\n",
        "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.openrouter import OpenRouter\n",
        "\n",
        "print(\"âœ… All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOyvKn1Crcx4"
      },
      "source": [
        "## ðŸ¤– Part 2: RAG Backend Class\n",
        "\n",
        "Create a simple RAG backend that can initialize the database and answer queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LB20251210:  Added based on ChatGPT based debug\n",
        "from llama_index.core.llms import CustomLLM\n",
        "import requests\n",
        "\n",
        "class OpenRouterLLM(CustomLLM):\n",
        "    def __init__(self, api_key, model=\"gpt-4o\", temperature=0.2):\n",
        "        self.api_key = api_key\n",
        "        self.model = model\n",
        "        self.temperature = temperature\n",
        "\n",
        "    @property\n",
        "    def metadata(self):\n",
        "        return {}\n",
        "\n",
        "    def complete(self, prompt, **kwargs):\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
        "            \"HTTP-Referer\": \"http://localhost\",\n",
        "            \"X-Title\": \"LlamaIndex-RAG\"\n",
        "        }\n",
        "\n",
        "        payload = {\n",
        "            \"model\": self.model,\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "            \"temperature\": self.temperature\n",
        "        }\n",
        "\n",
        "        response = requests.post(\n",
        "            \"https://openrouter.ai/api/v1/chat/completions\",\n",
        "            headers=headers,\n",
        "            json=payload,\n",
        "            timeout=60\n",
        "        )\n",
        "\n",
        "        data = response.json()\n",
        "        return data[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "print(\"âœ… OpenRouter API setup successfully!\")"
      ],
      "metadata": {
        "id": "q_Hch5SY1t3w",
        "outputId": "f485b4c6-774b-4b39-eea0-6a0ccfe24bdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… OpenRouter API setup successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LB20251210:  Added based on ChatGPT based debug: For OpenRouter API fallback for 401/439/500\n",
        "import requests\n",
        "from llama_index.core.llms import CustomLLM\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "\n",
        "class OpenRouterLLMWithFallback(CustomLLM):\n",
        "    def __init__(self, api_key, model=\"gpt-4o\", temperature=0.2):\n",
        "        self.api_key = api_key\n",
        "        self.model = model\n",
        "        self.temperature = temperature\n",
        "\n",
        "        # --- Load HuggingFace fallback ---\n",
        "        self.fallback_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "        self.fallback_model = AutoModelForCausalLM.from_pretrained(\n",
        "            \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def metadata(self):\n",
        "        return {}\n",
        "\n",
        "    def complete(self, prompt, **kwargs):\n",
        "        \"\"\"\n",
        "        Try OpenRouter. If it fails â†’ run local HuggingFace LLM.\n",
        "        \"\"\"\n",
        "        # -------------- Try OpenRouter --------------\n",
        "        try:\n",
        "            headers = {\n",
        "                \"Authorization\": f\"Bearer {self.api_key}\",\n",
        "                \"HTTP-Referer\": \"http://localhost\",\n",
        "                \"X-Title\": \"LlamaIndex-RAG\"\n",
        "            }\n",
        "\n",
        "            payload = {\n",
        "                \"model\": self.model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"temperature\": self.temperature\n",
        "            }\n",
        "\n",
        "            response = requests.post(\n",
        "                \"https://openrouter.ai/api/v1/chat/completions\",\n",
        "                headers=headers,\n",
        "                json=payload,\n",
        "                timeout=30\n",
        "            )\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                return data[\"choices\"][0][\"message\"][\"content\"]\n",
        "            else:\n",
        "                print(f\"[âš ï¸ OpenRouter Error] {response.status_code}: {response.text}\")\n",
        "                raise RuntimeError(\"Primary LLM failed\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[âš ï¸ Falling back to HuggingFace] {e}\")\n",
        "\n",
        "        # -------------- FALLBACK: HuggingFace local LLM --------------\n",
        "        inputs = self.fallback_tokenizer(prompt, return_tensors=\"pt\").to(self.fallback_model.device)\n",
        "        output = self.fallback_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            temperature=self.temperature\n",
        "        )\n",
        "        return self.fallback_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"âœ… HuggingFace fallback for OpenRouter API failure for 401/429/500 setup successfully!\")\n"
      ],
      "metadata": {
        "id": "Ki_9fEYC3on2",
        "outputId": "b6e7c28e-fb44-4922-b43a-c484268ad353",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… HuggingFace fallback for OpenRouter API failure for 401/429/500 setup successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LB20251210:  Added based on ChatGPT based debug: For OpenRouter API fallback for 401/439/500 with Stream support\n",
        "from llama_index.core.llms import CustomLLM, CompletionResponse, CompletionResponseGen\n",
        "import requests\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "\n",
        "class OpenRouterLLMWithFallback(CustomLLM):\n",
        "    def __init__(self, api_key, model=\"gpt-4o\", temperature=0.2):\n",
        "        self.api_key = api_key\n",
        "        self.model = model\n",
        "        self.temperature = temperature\n",
        "\n",
        "        # Load HuggingFace fallback\n",
        "        self.fallback_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "        self.fallback_model = AutoModelForCausalLM.from_pretrained(\n",
        "            \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def metadata(self):\n",
        "        return {\"model_name\": f\"OpenRouter({self.model}) + HF-Fallback\"}\n",
        "\n",
        "    # ----------------------------------------------------\n",
        "    # MAIN NON-STREAMING COMPLETION\n",
        "    # ----------------------------------------------------\n",
        "    def complete(self, prompt, **kwargs) -> CompletionResponse:\n",
        "        \"\"\"\n",
        "        Try OpenRouter â†’ if it fails, fallback to local HuggingFace.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                \"Authorization\": f\"Bearer {self.api_key}\",\n",
        "                \"HTTP-Referer\": \"http://localhost\",\n",
        "                \"X-Title\": \"LlamaIndex-RAG\"\n",
        "            }\n",
        "\n",
        "            payload = {\n",
        "                \"model\": self.model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"temperature\": self.temperature\n",
        "            }\n",
        "\n",
        "            response = requests.post(\n",
        "                \"https://openrouter.ai/api/v1/chat/completions\",\n",
        "                headers=headers,\n",
        "                json=payload,\n",
        "                timeout=25\n",
        "            )\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                text = data[\"choices\"][0][\"message\"][\"content\"]\n",
        "                return CompletionResponse(text=text)\n",
        "\n",
        "            # If request fails â†’ fallback\n",
        "            raise RuntimeError(f\"OpenRouter error: {response.status_code}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[âš ï¸ OpenRouter failed â†’ fallback triggered] {e}\")\n",
        "\n",
        "        # --------- FALLBACK LOCAL LLM ---------\n",
        "        inputs = self.fallback_tokenizer(prompt, return_tensors=\"pt\").to(self.fallback_model.device)\n",
        "        outputs = self.fallback_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=250,\n",
        "            temperature=self.temperature\n",
        "        )\n",
        "        text = self.fallback_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return CompletionResponse(text=text)\n",
        "\n",
        "    # ----------------------------------------------------\n",
        "    # STREAMING VERSION (required by CustomLLM)\n",
        "    # ----------------------------------------------------\n",
        "    def stream_complete(self, prompt, **kwargs) -> CompletionResponseGen:\n",
        "        \"\"\"\n",
        "        Minimal streaming implementation: just yields the full response once.\n",
        "        \"\"\"\n",
        "        full_response = self.complete(prompt).text\n",
        "        yield full_response\n",
        "print(\"âœ… HuggingFace fallback for OpenRouter API failure for 401/429/500 setup successfully with minimal STREAM support!\")\n"
      ],
      "metadata": {
        "id": "6wCdT5327UAn",
        "outputId": "61d71318-850a-4f8e-943d-6e610793f80f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… HuggingFace fallback for OpenRouter API failure for 401/429/500 setup successfully with minimal STREAM support!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "mEAkr1e2rcx4",
        "outputId": "465d9d6d-11c8-44a4-c311-055321e814c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "\"OpenRouterLLMWithFallback\" object has no field \"api_key\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2515522058.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m# Initialize the backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mrag_backend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleRAGBackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ðŸš€ RAG Backend initialized and ready!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2515522058.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetup_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2515522058.py\u001b[0m in \u001b[0;36msetup_settings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m#Settings.llm = OpenRouter(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m#LB20251210:  Fall back added based on debug with help of ChatGPT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             Settings.llm = OpenRouterLLMWithFallback(\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4o\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1534381932.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, model, temperature)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mOpenRouterLLMWithFallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCustomLLM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4o\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1030\u001b[0m                 \u001b[0msetattr_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m             \u001b[0;31m# if None is returned from _setattr_handler, the attribute was set directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msetattr_handler\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m                 \u001b[0msetattr_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# call here to not memo on possibly unknown fields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_setattr_handlers__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetattr_handler\u001b[0m  \u001b[0;31m# memoize the handler for faster access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m_setattr_handler\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'extra'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'allow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                     \u001b[0;31m# TODO - matching error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\"{cls.__name__}\" object has no field \"{name}\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                     \u001b[0;31m# attribute does not exist, so put it in extra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: \"OpenRouterLLMWithFallback\" object has no field \"api_key\""
          ]
        }
      ],
      "source": [
        "class SimpleRAGBackend:\n",
        "    \"\"\"Simple RAG backend for Gradio frontend.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.index = None\n",
        "        self.setup_settings()\n",
        "\n",
        "    def setup_settings(self):\n",
        "        \"\"\"Configure LlamaIndex settings.\"\"\"\n",
        "        # Set up the LLM using OpenRouter\n",
        "        api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "        if api_key:\n",
        "            #Settings.llm = OpenRouter(\n",
        "            #LB20251210:  Fall back added based on debug with help of ChatGPT\n",
        "            Settings.llm = OpenRouterLLMWithFallback(\n",
        "                api_key=api_key,\n",
        "                model=\"gpt-4o\",\n",
        "                temperature=0.1\n",
        "            )\n",
        "\n",
        "        # Set up the embedding model\n",
        "        Settings.embed_model = HuggingFaceEmbedding(\n",
        "            model_name=\"BAAI/bge-small-en-v1.5\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Set chunking parameters\n",
        "        Settings.chunk_size = 512\n",
        "        Settings.chunk_overlap = 50\n",
        "\n",
        "    def initialize_database(self, data_folder=\"/content/ai-accelerator-C3/Day_7/session_2/data\"):\n",
        "        \"\"\"Initialize the vector database with documents.\"\"\"\n",
        "        # Check if data folder exists\n",
        "        if not Path(data_folder).exists():\n",
        "            return f\"âŒ Data folder '{data_folder}' not found!\"\n",
        "\n",
        "        try:\n",
        "            # Create vector store\n",
        "            vector_store = LanceDBVectorStore(\n",
        "                uri=\"/content/ai-accelerator-C3/Day_7/session_2/basic_rag_vectordb\",\n",
        "                table_name=\"documents\"\n",
        "            )\n",
        "\n",
        "            # Load documents\n",
        "            reader = SimpleDirectoryReader(input_dir=data_folder, recursive=True)\n",
        "            documents = reader.load_data()\n",
        "\n",
        "            # Create storage context and index\n",
        "            storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "            self.index = VectorStoreIndex.from_documents(\n",
        "                documents,\n",
        "                storage_context=storage_context,\n",
        "                show_progress=True\n",
        "            )\n",
        "\n",
        "            return f\"âœ… Database initialized successfully with {len(documents)} documents!\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"âŒ Error initializing database: {str(e)}\"\n",
        "\n",
        "    def query(self, question):\n",
        "        \"\"\"Query the RAG system and return response.\"\"\"\n",
        "        # Check if index exists\n",
        "        if self.index is None:\n",
        "            return \"âŒ Please initialize the database first!\"\n",
        "\n",
        "        # Check if question is empty\n",
        "        if not question or not question.strip():\n",
        "            return \"âš ï¸ Please enter a question first!\"\n",
        "\n",
        "        try:\n",
        "            # Create query engine and get response\n",
        "            query_engine = self.index.as_query_engine()\n",
        "            response = query_engine.query(question)\n",
        "            return str(response)\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"âŒ Error processing query: {str(e)}\"\n",
        "\n",
        "# Initialize the backend\n",
        "rag_backend = SimpleRAGBackend()\n",
        "print(\"ðŸš€ RAG Backend initialized and ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS2i5epFrcx5"
      },
      "source": [
        "## ðŸŽ¨ Part 3: Gradio Interface\n",
        "\n",
        "Create a simple Gradio interface with:\n",
        "1. Button to initialize the database\n",
        "2. Text input for queries\n",
        "3. Button to submit queries\n",
        "4. Text output for responses\n",
        "5. Text output for status messages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1zDcw6cPrcx6",
        "outputId": "2e51fab6-af08-4179-8ef8-5eadd0346d04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Basic RAG interface created successfully!\n"
          ]
        }
      ],
      "source": [
        "def create_basic_rag_interface():\n",
        "    \"\"\"Create basic RAG interface with essential features.\"\"\"\n",
        "\n",
        "    def initialize_db():\n",
        "        \"\"\"Handle database initialization.\"\"\"\n",
        "        return rag_backend.initialize_database()\n",
        "\n",
        "    def handle_query(question):\n",
        "        \"\"\"Handle user queries.\"\"\"\n",
        "        return rag_backend.query(question)\n",
        "\n",
        "    # TODO: Create Gradio interface using gr.Blocks()\n",
        "    # Hint: Look at the structure below and fill in the missing components\n",
        "\n",
        "    with gr.Blocks(title=\"Basic RAG Assistant\") as interface:\n",
        "        # TODO: Add title and description\n",
        "        # Hint: Use gr.Markdown() for formatted text\n",
        "        # LB20251210\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            # ðŸ“˜ Basic RAG Assistant\n",
        "            This interface lets you **initialize the RAG database** and **run queries** against it.\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        # TODO: Add initialization section\n",
        "        # Hint: You need to use gr.Button to initialize the database\n",
        "        # init_btn = ?\n",
        "        # LB20251210\n",
        "        gr.Markdown(\"### ðŸ“¦ Initialize Database\")\n",
        "        init_btn = gr.Button(\"Initialize Database\")\n",
        "\n",
        "        # TODO: Add status output\n",
        "        # Hint: You need to use gr.Textbox to display the status\n",
        "        # LB20251210\n",
        "        status_output = gr.Textbox(\n",
        "            label=\"Status\",\n",
        "            placeholder=\"Click 'Initialize Database' to begin...\"\n",
        "        )\n",
        "\n",
        "        # The connection between the button and the status output has already been implemented\n",
        "        # at the end of this function\n",
        "\n",
        "        #  status_output = ?\n",
        "        # LB20251210\n",
        "        status_output = gr.Textbox(\n",
        "            label=\"Status\",\n",
        "            placeholder=\"Click 'Initialize Database' to begin...\"\n",
        "        )\n",
        "\n",
        "        # TODO: Add query section\n",
        "        # Hint: You need a text input, submit button, and response output\n",
        "        # LB20251210\n",
        "        gr.Markdown(\"### ðŸ’¬ Ask a Question\")\n",
        "\n",
        "        # Use gr.Textbox to create a text input\n",
        "        # query_input = ?\n",
        "        # LB20251210\n",
        "        query_input = gr.Textbox(\n",
        "            label=\"Your Question\",\n",
        "            placeholder=\"Type your question here...\"\n",
        "        )\n",
        "\n",
        "        # Use gr.Button to create a submit button\n",
        "        # submit_btn = ?\n",
        "        # LB20251210\n",
        "        submit_btn = gr.Button(\"Submit Query\")\n",
        "\n",
        "        # Use gr.Textbox to create a response output\n",
        "        # response_output = ?\n",
        "        # LB20251210\n",
        "        response_output = gr.Textbox(\n",
        "            label=\"Response\",\n",
        "            placeholder=\"RAG response will appear here...\"\n",
        "        )\n",
        "\n",
        "        # Connect buttons to functions\n",
        "        # Uncomment when above is implemented\n",
        "        # init_btn.click(initialize_db, outputs=[status_output])\n",
        "        # submit_btn.click(handle_query, inputs=[query_input], outputs=[response_output])\n",
        "        # LB20251210\n",
        "        init_btn.click(initialize_db, outputs=[status_output])\n",
        "        submit_btn.click(handle_query, inputs=[query_input], outputs=[response_output])\n",
        "\n",
        "    return interface\n",
        "\n",
        "# Create the interface\n",
        "basic_interface = create_basic_rag_interface()\n",
        "print(\"âœ… Basic RAG interface created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYR_1zvVrcx7"
      },
      "source": [
        "## ðŸš€ Part 4: Launch Your Application\n",
        "\n",
        "Launch your Gradio application and test it!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "zp0TZ2Cbrcx8",
        "outputId": "f26579a2-5dcc-4f72-9849-6a289bc3883f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ‰ Launching your Basic RAG Assistant...\n",
            "ðŸ”— Your application will open in a new browser tab!\n",
            "\n",
            "ðŸ“‹ Testing Instructions:\n",
            "1. Click 'Initialize Database' button first\n",
            "2. Wait for success message\n",
            "3. Enter a question in the query box\n",
            "4. Click 'Ask Question' to get AI response\n",
            "\n",
            "ðŸ’¡ Example questions to try:\n",
            "- What are the main topics in the documents?\n",
            "- Summarize the key findings\n",
            "- Explain the methodology used\n",
            "\n",
            "ðŸš€ Launch your app:\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://0c70fca338df84a5aa.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0c70fca338df84a5aa.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "print(\"ðŸŽ‰ Launching your Basic RAG Assistant...\")\n",
        "print(\"ðŸ”— Your application will open in a new browser tab!\")\n",
        "print(\"\")\n",
        "print(\"ðŸ“‹ Testing Instructions:\")\n",
        "print(\"1. Click 'Initialize Database' button first\")\n",
        "print(\"2. Wait for success message\")\n",
        "print(\"3. Enter a question in the query box\")\n",
        "print(\"4. Click 'Ask Question' to get AI response\")\n",
        "print(\"\")\n",
        "print(\"ðŸ’¡ Example questions to try:\")\n",
        "print(\"- What are the main topics in the documents?\")\n",
        "print(\"- Summarize the key findings\")\n",
        "print(\"- Explain the methodology used\")\n",
        "print(\"\")\n",
        "print(\"ðŸš€ Launch your app:\")\n",
        "\n",
        "# Your launch code here:\n",
        "# Uncomment when implemented\n",
        "# basic_interface.launch()\n",
        "# LB20251210\n",
        "basic_interface.launch()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LB20251210:  Debug\n",
        "os.getenv(\"OPENROUTER_API_KEY\")"
      ],
      "metadata": {
        "id": "xNc-oIPY0E-0",
        "outputId": "337f5f08-d211-4f3d-dd69-52ef1fc9c037",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'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'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "1. With OutSkill provider OpenRouter key, following error encountered at the lsat stage.\n",
        "âŒ Error processing query: Error code: 401 - {'error': {'message': 'User not found.', 'code': 401}}\n",
        "2.  With personal OpenRouter key, works fine."
      ],
      "metadata": {
        "id": "_8VBUOu_y-Ia"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMGPcBcUrcx8"
      },
      "source": [
        "## âœ… Assignment Completion Checklist\n",
        "\n",
        "Before submitting, ensure you have:\n",
        "\n",
        "- [x] RAG backend is provided and working\n",
        "- [x] Created Gradio interface with required components:\n",
        "  - [x] Title and description using gr.Markdown()\n",
        "  - [] Initialize database button using gr.Button()\n",
        "  - [ ] Status output using gr.Textbox()\n",
        "  - [ ] Query input field using gr.Textbox()\n",
        "  - [ ] Submit query button using gr.Button()\n",
        "  - [ ] Response output area using gr.Textbox()\n",
        "- [ ] Connected buttons to backend functions using .click()\n",
        "- [ ] Successfully launched the application\n",
        "- [ ] Tested the full workflow (initialize â†’ query â†’ response)\n",
        "\n",
        "## ðŸŽŠ Congratulations!\n",
        "\n",
        "You've successfully built your first Gradio RAG application! You now have:\n",
        "\n",
        "- A functional web interface for your RAG system\n",
        "- Understanding of Gradio basics and component connections\n",
        "- A foundation for building more complex AI applications\n",
        "\n",
        "**Next Steps**: Complete Assignment 3b to add advanced configuration options to your RAG interface!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "accelerator",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}