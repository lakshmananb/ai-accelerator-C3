{
  "name": "Semantic Cache with a Redis Vector Store",
  "nodes": [
    {
      "parameters": {
        "content": "### Tuning the Cache\nAdjust the `distanceThreshold` in the `Analyze results from store` node to control cache sensitivity:\n- **Lower threshold** (e.g., 0.2): More strict matching, fewer false positives, more LLM calls\n- **Higher threshold** (e.g., 0.5): More lenient matching, more cache hits, potential for less relevant responses",
        "height": 136,
        "width": 736,
        "color": 5
      },
      "id": "fe55ecfc-9013-495e-8ef4-695e33ee4a8d",
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        720,
        768
      ],
      "typeVersion": 1
    },
    {
      "parameters": {
        "model": {
          "__rl": true,
          "mode": "list",
          "value": "gpt-4.1-mini"
        },
        "options": {}
      },
      "id": "d30cc7ef-b4e1-435f-9a76-33fc5ce59b9a",
      "name": "OpenAI Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "position": [
        1632,
        592
      ],
      "typeVersion": 1.2
    },
    {
      "parameters": {},
      "id": "a5a3f004-5393-4370-a922-7a71bd9c0e14",
      "name": "Redis Chat Memory",
      "type": "@n8n/n8n-nodes-langchain.memoryRedisChat",
      "position": [
        1808,
        592
      ],
      "typeVersion": 1.5
    },
    {
      "parameters": {
        "options": {
          "responseMode": "responseNodes"
        }
      },
      "id": "fe910032-7940-4c51-a293-d6ddf174d5fd",
      "name": "When chat message received",
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "position": [
        640,
        224
      ],
      "webhookId": "113bee32-fc5f-4b90-98f0-dfad39a0d1c1",
      "typeVersion": 1.4
    },
    {
      "parameters": {
        "jsCode": "// Modify this to tweak the ratio between false positives and false negatives\nconst distanceThreshold = 0.3; \n// Step 1. - find all documents that score below the threshold\nconst allMatches = $input.all().filter(item => item.json.score < distanceThreshold);\n// Step 2. - choose the one with the best score\nreturn allMatches.length > 1 ? allMatches.reduce((min, item) => item.json.score < min.json.score ? item : min) : allMatches;\n// AT THIS POINT ONLY ONE (OR ZERO) DOCUMENTS WOULD PASS\n// 1 DOCUMENT - document with highest score, above the score threshold (cache hit)\n// 0 DOCUMENTS - none of the documents are above the score threshold (cache miss)"
      },
      "id": "dbbe6910-89ad-4f3b-a830-7d3e91eb3ba9",
      "name": "Analyze results from store",
      "type": "n8n-nodes-base.code",
      "position": [
        1216,
        224
      ],
      "executeOnce": true,
      "typeVersion": 2,
      "alwaysOutputData": true
    },
    {
      "parameters": {
        "mode": "load",
        "redisIndex": {
          "__rl": true,
          "mode": "list",
          "value": "chat_cache"
        },
        "prompt": "={{ $json.chatInput }}",
        "options": {}
      },
      "id": "da7b5667-e9de-4579-b9a4-04d0469e390f",
      "name": "Check for similar prompts",
      "type": "@n8n/n8n-nodes-langchain.vectorStoreRedis",
      "position": [
        864,
        224
      ],
      "typeVersion": 1.3
    },
    {
      "parameters": {
        "message": "={{ $json.document.metadata.reply }}",
        "waitUserReply": false,
        "options": {}
      },
      "id": "4c02ff5c-1e04-4b10-98d0-7e1d26b50f18",
      "name": "Respond to Chat (from semantic cache)",
      "type": "@n8n/n8n-nodes-langchain.chat",
      "position": [
        1712,
        64
      ],
      "typeVersion": 1,
      "alwaysOutputData": false
    },
    {
      "parameters": {
        "message": "={{ $json.metadata.reply }}",
        "waitUserReply": false,
        "options": {}
      },
      "id": "dd2e5e1e-63f0-4b7e-b71f-722a22a5cb96",
      "name": "Respond to Chat (from LLM)",
      "type": "@n8n/n8n-nodes-langchain.chat",
      "position": [
        2512,
        368
      ],
      "typeVersion": 1
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "={{ $('When chat message received').item.json.chatInput }}",
        "options": {
          "systemMessage": "You are a helpful assistant helping out with generic questions. Reply to the user with your best answer and don't invent much.",
          "maxIterations": 10
        }
      },
      "id": "6fcaba23-333c-460a-8c82-3ebe2d5c0dcd",
      "name": "LLM Agent",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "position": [
        1664,
        368
      ],
      "typeVersion": 1.8
    },
    {
      "parameters": {
        "mode": "insert",
        "redisIndex": {
          "__rl": true,
          "mode": "list",
          "value": "chat_cache",
          "cachedResultName": "chat_cache"
        },
        "options": {
          "overwriteDocuments": true
        }
      },
      "id": "fe86702f-6bab-4325-b52b-2898c6dac174",
      "name": "Store entry in cache",
      "type": "@n8n/n8n-nodes-langchain.vectorStoreRedis",
      "position": [
        2048,
        368
      ],
      "typeVersion": 1.3
    },
    {
      "parameters": {
        "jsonMode": "expressionData",
        "jsonData": "={{ $('When chat message received').item.json.chatInput }}",
        "options": {
          "metadata": {
            "metadataValues": [
              {
                "name": "reply",
                "value": "={{ $json.output }}"
              }
            ]
          }
        }
      },
      "id": "aa0f5570-0962-4d2e-aa57-acba5c677939",
      "name": "Add response as metadata",
      "type": "@n8n/n8n-nodes-langchain.documentDefaultDataLoader",
      "position": [
        2144,
        592
      ],
      "typeVersion": 1
    },
    {
      "parameters": {
        "options": {}
      },
      "id": "14434ff9-0659-44a5-965f-29fb284ea706",
      "name": "Recursive Character Text Splitter",
      "type": "@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter",
      "position": [
        2240,
        768
      ],
      "typeVersion": 1
    },
    {
      "parameters": {
        "content": "### Embedding model\nObviously using your own model to calculate the embeddings would not only increase performance but may also drastically reduce the costs.\n\nEven with the existing popular models though calling an embedding model is still much more cheaper than calling the chat model.",
        "height": 152,
        "width": 576,
        "color": 7
      },
      "id": "c7a4cd5d-dbbc-4474-a541-1dbbc557caef",
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        2096,
        16
      ],
      "typeVersion": 1
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "version": 2,
            "leftValue": "",
            "caseSensitive": true,
            "typeValidation": "strict"
          },
          "combinator": "and",
          "conditions": [
            {
              "id": "c59204e1-85b1-4d40-89ca-04718c693b36",
              "operator": {
                "type": "object",
                "operation": "exists",
                "singleValue": true
              },
              "leftValue": "={{ $json.document }}",
              "rightValue": ""
            }
          ]
        },
        "options": {}
      },
      "id": "12daff3e-2ed0-4aa4-928b-a5d77d82b5c9",
      "name": "Is this a cache hit?",
      "type": "n8n-nodes-base.if",
      "position": [
        1440,
        224
      ],
      "typeVersion": 2.2,
      "alwaysOutputData": false
    },
    {
      "parameters": {
        "options": {}
      },
      "id": "bdece50d-059d-46c2-b378-eef06aeeeefb",
      "name": "Embeddings HuggingFace Inference",
      "type": "@n8n/n8n-nodes-langchain.embeddingsHuggingFaceInference",
      "position": [
        864,
        464
      ],
      "typeVersion": 1
    },
    {
      "parameters": {
        "options": {}
      },
      "id": "c48d29fe-aa0a-4abb-abba-81e6ce0074bf",
      "name": "Embeddings HuggingFace Inference1",
      "type": "@n8n/n8n-nodes-langchain.embeddingsHuggingFaceInference",
      "position": [
        2000,
        592
      ],
      "typeVersion": 1
    }
  ],
  "pinData": {},
  "connections": {
    "LLM Agent": {
      "main": [
        [
          {
            "node": "Store entry in cache",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "OpenAI Chat Model": {
      "ai_languageModel": [
        [
          {
            "node": "LLM Agent",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Redis Chat Memory": {
      "ai_memory": [
        [
          {
            "node": "LLM Agent",
            "type": "ai_memory",
            "index": 0
          }
        ]
      ]
    },
    "Is this a cache hit?": {
      "main": [
        [
          {
            "node": "Respond to Chat (from semantic cache)",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "LLM Agent",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Store entry in cache": {
      "main": [
        [
          {
            "node": "Respond to Chat (from LLM)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Add response as metadata": {
      "ai_document": [
        [
          {
            "node": "Store entry in cache",
            "type": "ai_document",
            "index": 0
          }
        ]
      ]
    },
    "Check for similar prompts": {
      "main": [
        [
          {
            "node": "Analyze results from store",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Analyze results from store": {
      "main": [
        [
          {
            "node": "Is this a cache hit?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "When chat message received": {
      "main": [
        [
          {
            "node": "Check for similar prompts",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Embeddings HuggingFace Inference": {
      "ai_embedding": [
        [
          {
            "node": "Check for similar prompts",
            "type": "ai_embedding",
            "index": 0
          }
        ]
      ]
    },
    "Embeddings HuggingFace Inference1": {
      "ai_embedding": [
        [
          {
            "node": "Store entry in cache",
            "type": "ai_embedding",
            "index": 0
          }
        ]
      ]
    },
    "Recursive Character Text Splitter": {
      "ai_textSplitter": [
        [
          {
            "node": "Add response as metadata",
            "type": "ai_textSplitter",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "edc1883b-845a-495f-8734-a6a6fde48f25",
  "meta": {
    "instanceId": "5adcf055af070598f43d6403984bb72f646a6684587b890d0694cbc7278091f8"
  },
  "id": "XcalXasKzn6sNclP",
  "tags": []
}
